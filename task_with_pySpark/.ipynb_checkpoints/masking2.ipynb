{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a0ef41d-590d-4742-a941-edc536b32ded",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faker in /opt/bitnami/python/lib/python3.11/site-packages (28.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.4 in /opt/bitnami/python/lib/python3.11/site-packages (from faker) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/bitnami/python/lib/python3.11/site-packages (from python-dateutil>=2.4->faker) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75bfb5a0-3332-4cb7-97a5-1fe74af60837",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StringType, DateType\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# from faker import Faker\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Initialize SparkSession\u001b[39;00m\n\u001b[1;32m      8\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder \\\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerateFakeData\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.shuffle.partitions\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m500\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.executor.memory\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m8g\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.executor.cores\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m16\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;241m.\u001b[39mgetOrCreate()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf,col, expr\n",
    "from pyspark.sql.types import StringType, DateType\n",
    "from faker import Faker\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GenerateFakeData\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"500\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.cores\", \"16\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a59b4d5b-82d2-4601-b7a1-6aaca08ad114",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Faker' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fake \u001b[38;5;241m=\u001b[39m \u001b[43mFaker\u001b[49m()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# UDFs for generating fake data\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_first_name\u001b[39m():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Faker' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "fake = Faker()\n",
    "\n",
    "# UDFs for generating fake data\n",
    "def generate_first_name():\n",
    "    return fake.first_name()\n",
    "\n",
    "def generate_last_name():\n",
    "    return fake.last_name()\n",
    "\n",
    "def generate_address():\n",
    "    return fake.address()\n",
    "\n",
    "def generate_dob():\n",
    "    return fake.date_of_birth(minimum_age=18, maximum_age=90).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "first_name_udf = udf(generate_first_name, StringType())\n",
    "last_name_udf = udf(generate_last_name, StringType())\n",
    "address_udf = udf(generate_address, StringType())\n",
    "dob_udf = udf(generate_dob, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c69dbaff-9aee-4c2f-b65f-628151f841bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_first_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m first_name_udf \u001b[38;5;241m=\u001b[39m udf(\u001b[43mgenerate_first_name\u001b[49m, StringType())\n\u001b[1;32m      2\u001b[0m last_name_udf \u001b[38;5;241m=\u001b[39m udf(generate_last_name, StringType())\n\u001b[1;32m      3\u001b[0m address_udf \u001b[38;5;241m=\u001b[39m udf(generate_address, StringType())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generate_first_name' is not defined"
     ]
    }
   ],
   "source": [
    "first_name_udf = udf(generate_first_name, StringType())\n",
    "last_name_udf = udf(generate_last_name, StringType())\n",
    "address_udf = udf(generate_address, StringType())\n",
    "dob_udf = udf(generate_dob, StringType())\n",
    "\n",
    "# Create a DataFrame with the required number of rows\n",
    "num_records = 1000  # Adjust based on the size needed\n",
    "df = spark.range(num_records) \\\n",
    "    .select(\n",
    "        first_name_udf().alias(\"first_name\"),\n",
    "        last_name_udf().alias(\"last_name\"),\n",
    "        address_udf().alias(\"address\"),\n",
    "        dob_udf().alias(\"dob\")\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efec035f-718d-404e-8346-70ca26a3eaa0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Repartition to balance the load\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mrepartition(\u001b[38;5;241m100\u001b[39m)  \u001b[38;5;66;03m# Increase partitions if necessary\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Write the DataFrame to CSV\u001b[39;00m\n\u001b[1;32m      5\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdbfs:/dbfs/Workspace/Users/kratikamahale_outlook.com#ext#@kratikamahaleoutlook.onmicrosoft.com/databricks_training/output2.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Use DBFS path for Databricks\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df = df.repartition(100)  # Increase partitions if necessary\n",
    "\n",
    "# Define output path inside the container\n",
    "output_path = \"/opt/notebooks/output/output2.csv\"  # Use a path inside the container\n",
    "\n",
    "# Write the DataFrame to CSV\n",
    "df.write.csv(output_path, header=True, mode=\"overwrite\")\n",
    "\n",
    "print(f\"Data saved at: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d60ab056-4409-4d3c-b329-2537c73657e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Path to the CSV file in DBFS\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Read the CSV file from DBFS\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mcsv(output_path, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Define the masking functions\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmask_name\u001b[39m(name):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# Path to the CSV file in DBFS\n",
    "\n",
    "\n",
    "# Read the CSV file from DBFS\n",
    "df = spark.read.csv(output_path, header=True, inferSchema=True)\n",
    "\n",
    "# Define the masking functions\n",
    "def mask_name(name):\n",
    "    return name[0] + \"*\" * (len(name) - 1)  # Mask all but the first character\n",
    "\n",
    "def mask_address(address):\n",
    "    return \"Address Masked\"\n",
    "\n",
    "# Register UDFs for masking\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "mask_name_udf = udf(mask_name, StringType())\n",
    "mask_address_udf = udf(mask_address, StringType())\n",
    "\n",
    "# Apply masking to the DataFrame\n",
    "masked_df = df.withColumn(\"first_name\", mask_name_udf(col(\"first_name\"))) \\\n",
    "              .withColumn(\"last_name\", mask_name_udf(col(\"last_name\"))) \\\n",
    "              .withColumn(\"address\", mask_address_udf(col(\"address\")))\n",
    "masked_df.show(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_df.coalesce(1)\n",
    "\n",
    "# Define output path inside the container\n",
    "output_path = \"/opt/notebooks/output/masked_output.csv\"  # Use a path inside the container\n",
    "\n",
    "# Write the DataFrame to CSV\n",
    "df.write.csv(output_path, header=True, mode=\"overwrite\")\n",
    "\n",
    "print(f\"Data saved at: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "masking2",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
